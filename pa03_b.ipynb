# EE532L - Deep Learning for Healthcare - Programming Assignment 03
# Authors: Jibitesh Saha, Sasidhar Alavala, Subrahmanyam Gorthi
# Important: Please do not change/rename the existing function names and write your code only in the place where you are asked to do it.

########################################################## Can be modified ##############################################################
# You can import libraries as per your need
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
import numpy as np
import matplotlib.pyplot as plt
##from sklearn.metrics import precision_score, recall_score, confusion_matrix


X_train = np.load('train_images.npy')
N1 = X_train.shape[0]
X_train = X_train.reshape([N1, 784])
X_train = X_train / 255.0  # Normalize training features

X_val = np.load('val_images.npy')
N2 = X_val.shape[0]
X_val = X_val.reshape([N2, 784])
X_val = X_val / 255.0  # Normalize validation features

X_test = np.load('test_images.npy')
N3 = X_test.shape[0]
X_test = X_test.reshape([N3, 784])
X_test = X_test / 255.0  # Normalize testing features

y_train = np.squeeze(np.load('train_labels.npy'))

y_val = np.squeeze(np.load('val_labels.npy'))

y_test = np.squeeze(np.load('test_labels.npy'))

# Neural Network
model = Sequential()
model.add(Dense(64, input_shape=(784,), activation="sigmoid"))  # 128 neurons in first hidden layer
model.add(Dense(32, activation="sigmoid"))  # 64 neurons in second hidden layer
model.add(Dense(16, activation="sigmoid"))
model.add(Dense(1, activation="sigmoid"))  # Output layer

opt = SGD(learning_rate=0.2)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', 'precision', 'recall', 'true_positives', 'true_negatives', 'false_positives', 'false_negatives'])

# Training and Validation
H = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_val, y_val), verbose=0)

# Testing
loss, accuracy, precision, recall, tp, tn, fp, fn = model.evaluate(X_test, y_test, batch_size=1, verbose=0)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

##precision = precision_score(y_test, y_pred)
##recall = recall_score(y_test, y_pred)
##tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
##accuracy = (tp + tn) / (tp + tn + fp + fn)

# Calculating Specificity
specificities = [tn / (tn + fp) for tn, fp in zip(H.history['true_negatives'], H.history['false_positives'])]

# Calculating F1 Score
f1_scores = [2 * (p * r) / (p + r) if p + r != 0 else 0 for p, r in zip(H.history['precision'], H.history['recall'])]



# Plotting
epochs = range(1, len(H.history['loss']) + 1)  # Number of epochs



plt.figure(figsize=(12, 6))

plt.subplot(2, 3, 1)
plt.plot(epochs, H.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs Epoch')

plt.subplot(2, 3, 2)
plt.plot(epochs, H.history['accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Epoch')

plt.subplot(2, 3, 3)
plt.plot(epochs, H.history['precision'])
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.title('Precision vs Epoch')

plt.subplot(2, 3, 4)
plt.plot(epochs, H.history['recall'])
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.title('Recall vs Epoch')

plt.subplot(2, 3, 5)
plt.plot(epochs, specificities)
plt.xlabel('Epoch')
plt.ylabel('Specificity')
plt.title('Specificity vs Epoch')

plt.subplot(2, 3, 6)
plt.plot(epochs, f1_scores)
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.title('F1 Score vs Epoch')

plt.tight_layout()
plt.show()



